{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn: Classification and Regression Models & Examples\n",
    "\n",
    "This notebook will explore various classification and regression techniques available through the scikit-learn python library. The techniques covered in this notebook are:\n",
    "- Nearest neighbor (NN)\n",
    "- Support vector machines (SVM)\n",
    "- Decision trees/random forrest\n",
    "- Linear discriminant analysis\n",
    "- Logistic regression\n",
    "\n",
    "The dataset that we will use are the MNIST handwritten digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "data = datasets.load_digits()\n",
    "images = data.images\n",
    "targets = data.target\n",
    "\n",
    "images_train, images_test, labels_train, labels_test = train_test_split(images, targets, test_size=.2, shuffle=False)\n",
    "\n",
    "image = np.asarray(images[2]).squeeze()\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "print(f\"This is a number {targets[2]}\\n\\n\")\n",
    "\n",
    "# # Re-shape data so that it's 2D\n",
    "images_train = np.reshape(images_train, (np.shape(images_train)[0], 64))\n",
    "images_test = np.reshape(images_test, (np.shape(images_test)[0], 64))\n",
    "\n",
    "print(f\"Shapes of our train/test data and labels:\")\n",
    "print(f\"\\timages_train: {np.shape(images_train)}\")\n",
    "print(f\"\\tlabels_train: {np.shape(labels_train)}\")\n",
    "print(f\"\\timages_test: {np.shape(images_test)}\")\n",
    "print(f\"\\tlabels_test: {np.shape(labels_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Nearest Neighbor Algorithm\n",
    "\n",
    "The nearest neighbor algorithm is one of the more basic classification techniques, and in some cases is actually used as a building block in other richer machine learning algorithms. It's simplicity, in some regard, is where it's power lies: it's fast to implement, and easy to understand. The algorithm looks at the Euclidean distance of given data point to it's nearest neighboring data points to help us gain an understanding of the information contained in the data point of interest. Data points with \"closer\" Euclidean distances are more likely to be similar than ones whose Euclidean distances are quite far. \n",
    "\n",
    "The nearest neighbor algorithm can come in a few different flavors.\n",
    "\n",
    "- **unsupervised**: we don't know the label of any of the neighboring data points\n",
    "\n",
    "- **supervised**: we do know the label of the neighboring data points\n",
    "\n",
    "    - **classification**: the data has a discrete and finite number of values it can take. In supervised nearest neighbor classification, we are typically aiming to predict the class of some data by taking a \"vote\" of the neighboring pixels. Which ever class receives the most votes, we classify our data as such\n",
    "    \n",
    "    - **regression**: the data can take any infinite number of values. In this scenario, we usually take an average of the values of the nearest neighbors\n",
    "    \n",
    "- **radius v.s. k-nearest neighbors**: this determines whether we are looking at a fixed number of neighbors, or all neighbors who are within a fixed distance\n",
    "\n",
    "\n",
    "Let's pick a sample image to plot the 9 nearest neighbors of it, so that we can get a feel for the ```neighbors``` interface and how we would use unsupervised learning to get a feel for our data, without actually know which labels the data corresponds to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a digit in the training set\n",
    "seed = 43\n",
    "random_digit = images_train[seed] # get an image from out training images\n",
    "random_digit_label = labels_train[seed] # get it's corresponding label\n",
    "\n",
    "#Plot the image and display it's label\n",
    "fig = plt.figure()\n",
    "plt.imshow(random_digit.reshape(8,8))\n",
    "plt.title(f\"This is a number {random_digit_label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can import ```NearestNeighbors```, which is the interface required when we are detecting nearest neighbors without their labels (ie. unsupervised)\n",
    "\n",
    "The key input parameters for the ```NearestNeighbors``` constructor is as follows:\n",
    "```NearestNeighbors(n_neighbors, algorithm).fit(data)```\n",
    "\n",
    "- ```n_neighbors```: number of neighbors we want to find, default: 5. You can also use ```radius``` instead to specify a radius instead of number of neighbors\n",
    "- ```algorithm```: which algorithm to use to search for the neighbors. Valid parameter values are ```auto```, ```ball_tree```, ```kd_tree```, ```brute```. Default is ```auto```, and that's what we will use in this notebook. The two tree searches are more efficient in certain scenarios which are beyond the scope of this notebook, so we'll let ```auto``` figure it out for us\n",
    "- ```.fit(data)```: the data set which will be used to find the nearest neighbors within\n",
    "\n",
    "After fitting the ```NearestNeighbors``` object to our dataset, we can call the object's ```.kneighbors``` method to specify the point who's neighbors we'd like to find, in our case the point ```random_digit```. This method will return two lists: one for the distances of the neighbors, and one for the indices in the neighbors in the input data that we ```.fit()``` our model to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Initialize our neighbors object\n",
    "nbrs = NearestNeighbors(n_neighbors=10, algorithm='auto').fit(images_train)\n",
    "# return the distances and indices of the neighbors nearest to seed\n",
    "distances, indices = nbrs.kneighbors(images_train[seed].reshape((1,-1)))\n",
    "\n",
    "# Plot the 9 nearest neighbors\n",
    "fig = plt.figure()\n",
    "for i in range(1,10):\n",
    "    img = np.reshape(images_train[indices[0][i], :], (8,8))\n",
    "    fig.add_subplot(3, 3, i)\n",
    "    plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN classifier\n",
    "\n",
    "Let's use the supervised classifier, KNeighborsClassifier, to get a feel for how all of this works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "nbrs_k = KNeighborsClassifier(n_neighbors=20, weights='uniform', algorithm='auto').fit(images_train, labels_train)\n",
    "prediction = nbrs_k.predict(images_train[seed].reshape(1,-1))\n",
    "print(f\"Predicted class: {prediction[0]}\")\n",
    "print(f\"Actual class: {labels_train[seed]}\")\n",
    "\n",
    "# See if there were any other digits the algorithm thought it could be\n",
    "print(f\"Probability of each class for this example: {nbrs_k.predict_proba(images_train[seed].reshape(1,-1))[0]}\\n\\n\")\n",
    "\n",
    "# Now lets do this for each data in our test set, to see how accurate KNN classifier can be\n",
    "nbrs_k = KNeighborsClassifier(n_neighbors=20, weights='uniform', algorithm='auto').fit(images_test, labels_test)\n",
    "\n",
    "# We'll also run a weighted version to see how much this helps (if at all)\n",
    "nbrs_k_weighted = KNeighborsClassifier(n_neighbors=20, weights='distance', algorithm='auto').fit(images_test, labels_test) \n",
    "\n",
    "# Some variables to help keep track of how we're doing\n",
    "number_correct = 0\n",
    "number_correct_weighted = 0\n",
    "\n",
    "# Loop over all \n",
    "for index_test in range(len(labels_test)):\n",
    "    \n",
    "    prediction = nbrs_k.predict(images_test[index_test].reshape(1,-1))\n",
    "    if prediction[0] == labels_test[index_test]:\n",
    "        number_correct += 1\n",
    "    \n",
    "    prediction_weighted = nbrs_k_weighted.predict(images_test[index_test].reshape(1,-1))\n",
    "    if prediction_weighted[0] == labels_test[index_test]:\n",
    "        number_correct_weighted += 1\n",
    "print(\"Results from running this on all test data...\")\n",
    "print(f\"\\tPercent correct for all test data: {100*number_correct/len(labels_test)}%\")\n",
    "print(f\"\\tPercent correct for all test data (weighted): {100*number_correct_weighted/len(labels_test)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! Can't beat 100%. Weighting the neighbors by distance ended up working well here, but this is guarunteed to be the case. It could be the case that the closer neighbors just so happen to be the wrong class label, in which case them getting weighted more might be detrimental instead of beneficial.\n",
    "\n",
    "### KNN Regression\n",
    "\n",
    "In case we had a dataset where the labels took on continuous values, we could have still used the nearest neighbors algorithm, and instead of invoking ```KNeighborsClassifier```, which predicts data into discrete classes, we could have used ```KNeighborsRegressor```. The regressor works by taking an average of the K nearest data points to predict the value of point of interest. The syntax is exactly the same for the regressor despite the two estimators targetting different applications.\n",
    "\n",
    "### KNN Nearest Centroid Classifier\n",
    "\n",
    "An alternative flavor of the supervised KNN classifier is to use nearest-centroid classification, meaning we will classify a data point as the label of the nearest centroid. Running this classifier is similar as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestCentroid\n",
    "\n",
    "nbrs_k_centroid = NearestCentroid().fit(images_train, labels_train)\n",
    "\n",
    "number_correct_centroid = 0\n",
    "\n",
    "# Loop over all \n",
    "for index_test in range(len(labels_test)):\n",
    "    \n",
    "    prediction_centroid = nbrs_k_centroid.predict(images_test[index_test].reshape(1,-1))\n",
    "    if prediction_centroid[0] == labels_test[index_test]:\n",
    "        number_correct_centroid += 1\n",
    "\n",
    "print(f\"Percent correct for all test data (centroid method): {100*number_correct_centroid/len(labels_test)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Naive Bayes Classifier\n",
    "\n",
    "Below: multinomial (several classes) is more accurate representation of the distribution than Gaussian (which is for continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB().fit(images_train, labels_train)\n",
    "\n",
    "number_correct_mnb = 0\n",
    "\n",
    "for index_test in range(len(labels_test)):\n",
    "    prediction_mnb = mnb.predict(images_test[index_test,:].reshape(1,-1))[0]\n",
    "    if prediction_mnb == labels_test[index_test]:\n",
    "        number_correct_mnb += 1\n",
    "\n",
    "print(\"Naive Bayes Classifier (multinomial distribution)...\")\n",
    "print(f\"\\tPercent correct for all test data: {100*number_correct_mnb/len(labels_test)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not quite as accurate on this particular data set as the nearest neighbor classifiers were. Naive Bayes models are simple and hence fast (both to develop and to compute a result), and they work relatively well even in a paucity of training data. Because of the core assumption made about the conditionally indepedent relationship between features, the Naive Bayes method also extends well to high dimensional data with a lot of features; the posterior distribution simply reduces to that many (assumingly) independent posteriors of a single variable. \n",
    "\n",
    "$P(x_i | y)$ was taken to be a multinomial distribution above. When that assumption doesn't hold, there are alternative distributions (such as Gaussian, complement, and Bernoulli) in the SciKit-Learn naive_bayes module which still permit the necessary assumptions of conditional independence.\n",
    "\n",
    "## 3. Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(max_iter=100000,multi_class='multinomial').fit(images_train,labels_train)\n",
    "print(lr.predict(images_test[0,:].reshape(1,-1)))\n",
    "print(lr.score(images_test, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
